import os
import json
import logging
import re
import aiohttp
import asyncio
from typing import Dict, List, Any, Optional, Union
import json
import logging
import re
from typing import Dict, List, Any, Optional, Union
from agents.evaluation_guidance_agent import EvaluationGuidanceAgent
from utils.llm_utils import query_llm, query_multiple_llms
from config import (
    LLM_API_URL, 
    LLM_API_KEY, 
    DEFAULT_MODEL,
    BACKUP_MODEL,
    TEMPERATURE,
    MAX_TOKENS,
    REQUEST_TIMEOUT
)

logger = logging.getLogger(__name__)


# Modified construct_evaluation_prompt function in llm_client.py

async def construct_evaluation_prompt(
    problem_statement: str,
    rubric: str,
    student_code: str,
    model_solution: Optional[str] = None,
    algorithm_guidance: Optional[Dict[str, Any]] = None,
    approach_explanation: Optional[str] = None,  # NEW parameter
    problem_dir: Optional[str] = None
) -> str:
    """
    Construct a secure prompt for code evaluation with algorithm-specific guidance
    generated by the EvaluationGuidanceAgent and approach explanation from ApproachExplanationAgent,
    leveraging example solutions when available
    
    Args:
        problem_statement: The problem statement
        rubric: The evaluation rubric
        student_code: The student's code submission
        model_solution: Optional model solution
        algorithm_guidance: Optional algorithm guidance from the guidance agent
        approach_explanation: Optional detailed explanation of student's approach
        problem_dir: Optional path to directory with example solutions
        
    Returns:
        Evaluation prompt for the LLM
    """
    # Create a general algorithm guidance as fallback
    general_guidance = """
    ALGORITHM EVALUATION GUIDANCE:
    - When evaluating algorithms, consider both explicit and implicit correctness
    - Many algorithms have multiple valid implementations that use different patterns
    - For search algorithms, pay attention to boundary conditions and edge cases
    - Consider the overall logic and structure of the solution, not just specific lines
    - A solution may be correct even if it implements the algorithm differently from what you expect
    - Focus on whether the solution produces correct results for all possible inputs
    """
    
    # Initialize specific_guidance
    specific_guidance = general_guidance
    
    # Use provided algorithm_guidance if available
    if algorithm_guidance:
        algorithm_type = algorithm_guidance.get("algorithm_type", "general")
        algorithm_guidance_text = algorithm_guidance.get("algorithm_guidance", "")
        test_cases = algorithm_guidance.get("test_cases", "")
        common_errors = algorithm_guidance.get("common_errors", "")
        key_patterns = algorithm_guidance.get("key_implementation_patterns", "")
        misleading_patterns = algorithm_guidance.get("misleading_patterns", "")
        
        specific_guidance = f"""
        ALGORITHM-SPECIFIC GUIDANCE FOR {algorithm_type.upper()}:
        {algorithm_guidance_text}
        
        KEY IMPLEMENTATION PATTERNS TO LOOK FOR:
        {key_patterns}
        
        MISLEADING PATTERNS TO IGNORE:
        {misleading_patterns}
        
        COMMON ERRORS TO WATCH FOR:
        {common_errors}
        
        TEST CASES FOR VERIFICATION:
        {test_cases}
        """
        
        logger.info(f"Using provided guidance for algorithm type: {algorithm_type}")
    # If no algorithm_guidance provided, generate it here
    else:
        # Existing code for generating guidance...
        pass
    
    # Include approach explanation if available
    student_approach_section = ""
    if approach_explanation:
        student_approach_section = f"""
        {approach_explanation}
        
        NOTE: The above analysis provides insight into the student's implementation approach 
        and identifies any issues without suggesting fixes. Use this information to better 
        understand what the student has attempted, but ensure your evaluation is based on 
        the actual code and the rubric criteria.
        """
    
    prompt = f"""
    You are a secure code evaluator with expertise in algorithms and programming languages.
    Your task is to evaluate student code based SOLELY on the provided rubric.
    Evaluate the code fairly and objectively, considering all valid implementation approaches.
    
    PROBLEM STATEMENT:
    ```
    {problem_statement}
    ```
    
    {specific_guidance}
    
    {student_approach_section}
    
    RUBRIC:
    ```
    {rubric}
    ```
    """
    
    if model_solution:
        prompt += f"""
        MODEL SOLUTION:
        ```
        {model_solution}
        ```
        """
    
    prompt += f"""
    STUDENT CODE:
    ```
    {student_code}
    ```
    
    EVALUATION INSTRUCTIONS:
    1. Evaluate the student code against each point in the rubric
    2. For each rubric point, determine if it is satisfied by the implementation
    3. Provide a brief, specific justification for each evaluation
    4. Consider ALL valid algorithmic approaches for solving this problem - there's often more than one correct way
    5. Calculate the total score based on marks awarded for each rubric point
    6. Focus on algorithmic correctness rather than syntax or style unless the rubric specifies otherwise
    7. Verify the code using the test cases provided above
    8. Mark the student's code based on what it DOES, not what you think the intention was
    9. IGNORE misleading comments - assess what the code actually implements, not what comments claim it does
    10. When comments and implementation conflict, trust the implementation
    
    CRITICAL: Comments and variable names may be deliberately misleading. Focus on the actual algorithm implementation 
    patterns, not descriptions. Trace the code execution to determine its real behavior.
    
    Return your evaluation as a JSON object with the following structure:
    {{
        "approach_used": "Which approach the student used (e.g., Solution 1, Solution 2, etc.)",
        "evaluation": {{
            "1": {{
                "satisfied": true/false,
                "justification": "Brief explanation with specific details from the code",
                "marks_awarded": marks_value
            }},
            "2": {{ ... }},
            ...
        }},
        "total_score": sum_of_marks,
        "max_possible_score": max_marks,
        "feedback": "Overall feedback on the submission highlighting strengths and areas for improvement"
    }}
    
    IMPORTANT REMINDER: 
    - Consider the problem-specific guidance above when evaluating this submission
    - There are often multiple valid implementations for the same algorithm
    - Focus on whether the solution correctly solves the problem, not whether it matches your preferred approach
    - If the implementation is correct but different from what you expected, it should still receive full marks
    - Comments and variable names may intentionally misrepresent the code's behavior - always analyze the actual logic
    """
    print(prompt)
    return prompt

def parse_llm_response(response_text: str) -> Dict[str, Any]:
    """
    Parse the LLM response text into structured data
    
    Args:
        response_text: Raw response from the LLM
        
    Returns:
        Parsed response as a dictionary
    """
    try:
        # Try to extract JSON from the response
        json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
            return json.loads(json_str)
        
        # If no JSON code block, try to parse the entire response
        return json.loads(response_text)
    except Exception as e:
        logger.error(f"Error parsing LLM response: {str(e)}", exc_info=True)
        logger.debug(f"Raw response: {response_text}")
        
        # Return a basic structure to prevent downstream errors
        return {
            "error": "Failed to parse LLM response",
            "approach_used": "unknown",
            "evaluation": {},
            "total_score": 0,
            "max_possible_score": 0,
            "feedback": "Error processing evaluation"
        }